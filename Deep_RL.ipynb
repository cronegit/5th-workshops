{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Deep-RL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cronegit/5th-workshops/blob/master/Deep_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfOuv88IHJTV",
        "colab_type": "text"
      },
      "source": [
        "##Temporal Difference y el Aprendizaje Reforzado\n",
        "\n",
        "![RL](https://www.researchgate.net/publication/322424392/figure/fig4/AS:667648979378190@1536191329604/Reinforcement-learning-schematic-Reinforcement-learning-RL-can-be-formulated-as-a.png)\n",
        "\n",
        "###Q-Learning\n",
        "Q-learning busca maximizar las acciones en base a una estimación de valor.\n",
        "\n",
        "El valor que utiliza es qué tan recompensatorio sería lo que podría producir cierta acción, en el futuro.\n",
        "\n",
        "![Ruta](https://miro.medium.com/max/1920/1*skxY50Y9pZNV2QZipmwkjg.png)\n",
        "\n",
        "\n",
        "![Reward delivery](https://miro.medium.com/max/1920/1*O7w5QJiZEp4LhCSV9Vi9ew.png)\n",
        "\n",
        "Este valor se estima a través de la propagación de la recompensa recibida en cierto estado.\n",
        "\n",
        "Es muy importante definir la información que se utilizará para establecer los estados, las acciones y las recompenzas.\n",
        "\n",
        "![RL](https://miro.medium.com/max/675/1*2aGjVH9aCRSXsMg1vT4peg.png)\n",
        "![estado-accion-recompensa](https://rubenlopezg.files.wordpress.com/2015/05/reinforcement1.png)\n",
        "\n",
        "\n",
        "A través del tiempo, el agente aprende las rutas que lo guían a mayores recompensas.\n",
        "\n",
        "![equations](https://wikimedia.org/api/rest_v1/media/math/render/svg/59db58edf1222b292e40706e503ed5974553606b)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3HGja08YJWr",
        "colab_type": "code",
        "outputId": "815e798b-460d-4244-d155-487ccd2ac60b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Youtube\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SIqXxtKtbtY?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SIqXxtKtbtY?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWMCvceUsqf4",
        "colab_type": "text"
      },
      "source": [
        "###Deep Q-Learning\n",
        "\n",
        "A medida que los problemas se vuelven más complejos, Q-Learning se enfrenta a poder rescatar de forma precisa la representación de los estados.\n",
        "\n",
        "El espacio de Go es de alrededor de 10 ^ 170 estados de tablero posibles.\n",
        "\n",
        "![Deep Q-Learning](https://miro.medium.com/max/2764/1*0_TNa54fr_LsLOllgIsrcw.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25Bg2hf0Y0J2",
        "colab_type": "code",
        "outputId": "cc9a43a0-da79-4709-e084-63457806dc0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Youtube\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LUEEmovuuX4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LUEEmovuuX4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt1vbmpdZaNc",
        "colab_type": "code",
        "outputId": "6a74127a-990d-4d30-a620-84f78dec06a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Youtube\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V1eYniJ0Rnk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V1eYniJ0Rnk?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62-bY3NhZp_Z",
        "colab_type": "code",
        "outputId": "8a801f45-9e16-44d7-d382-6e280b8446b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Youtube\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/C-WbsyfRc0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/C-WbsyfRc0w?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hXTjaJqNHJTa",
        "colab_type": "text"
      },
      "source": [
        "# Building an Agent to Play Miss PacMan games using Deep Q Network, in Tensorflow 2.0.\n",
        "\n",
        "Basado en el notebook de GradientCrescent, por Adrian Yijie Xu.\n",
        "[GradientCrescent](https://github.com/EXJUSTICE/GradientCrescent)\n",
        "\n",
        "## Training\n",
        "\n",
        "First we import all the necessary libraries </font> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms0B7dnLHJTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
        "from collections import deque, Counter\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dwBdXDvAe5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "wXDlF6cdHJTj",
        "colab_type": "text"
      },
      "source": [
        "Now we define a preprocessing function for our input game screens. We crop the image size convert the image into greyscale 1D tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29CSBYkPHJTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "color = np.array([210, 164, 74]).mean()\n",
        "\n",
        "\n",
        "#prepro (210, 160, 3) uint8 frame into 7040 (88x80) 1D float vector \n",
        "\n",
        "def preprocess_observation(obs):\n",
        "\n",
        "    # Crop and resize the image\n",
        "    img = obs[1:176:2, ::2]\n",
        "\n",
        "    # Convert the image to greyscale\n",
        "    img = img.mean(axis=2)\n",
        "\n",
        "    # Improve image contrast\n",
        "    img[img==color] = 0\n",
        "\n",
        "    # Next we normalize the image from -1 to +1\n",
        "    img = (img - 128) / 128 - 1\n",
        "\n",
        "    return img.reshape(88,80,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1qZntlnHJTs",
        "colab_type": "text"
      },
      "source": [
        "Let us initialize our gym environment, and take a look at some observations. Let's also inspect the size and type of action space in this game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnbrjWstHJTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"MsPacman-v0\")\n",
        "n_outputs = env.action_space.n\n",
        "print(n_outputs)\n",
        "print(env.env.get_action_meanings())\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for i in range(22):\n",
        "  \n",
        "  if i > 20:\n",
        "    plt.imshow(observation)\n",
        "    plt.show()\n",
        "\n",
        "  observation, _, _, _ = env.step(1)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ma31y4wFpst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Take a look at the preprocessed inputs in greyscale\n",
        "#Let's compare the original and preprocessed tensors.\n",
        "\n",
        "obs_preprocessed = preprocess_observation(observation).reshape(88,80)\n",
        "plt.imshow(obs_preprocessed)\n",
        "plt.show()\n",
        "print(observation.shape)\n",
        "print(obs_preprocessed.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "t5lhlUdqHJT2",
        "colab_type": "text"
      },
      "source": [
        "Okay, Now we define a function called q_network for building our Deep Q network.  <br>\n",
        "We build Q network with three convolutional layers with same padding followed by a flattening, and a fully connected layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4Cco6huHJT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "#Reset is technically not necessary if variables done  in TF2\n",
        "#https://github.com/ageron/tf2_course/issues/8\n",
        "\n",
        "def q_network(X, name_scope):\n",
        "    \n",
        "    # Initialize layers\n",
        "    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=2.0)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name_scope) as scope: \n",
        "\n",
        "\n",
        "        # initialize the convolutional layers\n",
        "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer) \n",
        "        tf.compat.v1.summary.histogram('layer_1',layer_1)\n",
        "        \n",
        "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('layer_2',layer_2)\n",
        "        \n",
        "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('layer_3',layer_3)\n",
        "        \n",
        "        # Flatten the result of layer_3 before feeding to the fully connected layer\n",
        "        flat = flatten(layer_3)\n",
        "        # Insert fully connected layer\n",
        "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('fc',fc)\n",
        "        #Add final output layer\n",
        "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('output',output)\n",
        "        \n",
        "\n",
        "        # Vars will store the parameters of the network such as weights\n",
        "        vars = {v.name[len(scope.name):]: v for v in tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} \n",
        "        #Return both variables and outputs together\n",
        "        return vars, output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsuM5sy3HJT9",
        "colab_type": "text"
      },
      "source": [
        "Next we define a function called epsilon_greedy for performing epsilon greedy policy.\n",
        "\n",
        "In epsilon greedy policy we either select the best action with probability [1 - epsilon] or a random action with\n",
        "probability [epsilon].\n",
        "\n",
        "\n",
        "We use decaying epsilon greedy policy where value of epsilon will be decaying over time as we don't want to explore\n",
        "forever. So over time our policy will be exploiting only good actions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSgZ4PHaHJT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.5\n",
        "eps_min = 0.05\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 500000\n",
        "\n",
        "#\n",
        "def epsilon_greedy(action, step):\n",
        "    p = np.random.random(1).squeeze() #1D entries returned using squeeze\n",
        "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps) #Decaying policy with more steps\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK5Ev_r-HJUD",
        "colab_type": "text"
      },
      "source": [
        "Now, we initialize our  buffer of length 20000 which holds the gameplay information in SARSA.\n",
        "\n",
        "We store all the agent's experience i.e (state, action, rewards) in the experience replay buffer\n",
        "and  we sample from this minibatch of experience for generating the y-values for the update function, and hence train the network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DU5VHc-HJUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buffer_len = 20000\n",
        "#Buffer is made from a deque - double ended queue\n",
        "exp_buffer = deque(maxlen=buffer_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-jwru2THJUI",
        "colab_type": "text"
      },
      "source": [
        "Next, we define a function called sample_memories for sampling experiences from the memory according to batches. Batch size is the number of experience sampled\n",
        "from the memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em_AW8I7HJUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_memories(batch_size):\n",
        "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
        "    mem = np.array(exp_buffer)[perm_batch]\n",
        "    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTRBSM53HJUM",
        "colab_type": "text"
      },
      "source": [
        "Now we define our network hyperparameters,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1iZveLKHJUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 800\n",
        "batch_size = 48\n",
        "input_shape = (None, 88, 80, 1)\n",
        "#Recall shape is img.reshape(88,80,1)\n",
        "learning_rate = 0.001\n",
        "X_shape = (None, 88, 80, 1)\n",
        "discount_factor = 0.97\n",
        "\n",
        "global_step = 0\n",
        "copy_steps = 100\n",
        "steps_train = 4\n",
        "start_steps = 2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-JwepmXHJUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logdir = 'logs'\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Now we define the placeholder for our input i.e game state\n",
        "X = tf.compat.v1.placeholder(tf.float32, shape=X_shape)\n",
        "\n",
        "# we define a boolean called in_training_model to toggle the training\n",
        "in_training_mode = tf.compat.v1.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3BS7lsPHJUT",
        "colab_type": "text"
      },
      "source": [
        " Now let us build our primary and target Q network We have two networks to allow for training and data generation to occur concurrently.\n",
        " Note that the network returns the weights, as well as network outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibqY86h4HJUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we build our Q network, which takes the input X and generates Q values for all the actions in the state\n",
        "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
        "\n",
        "# similarly we build our target Q network\n",
        "targetQ, targetQ_outputs = q_network(X, 'targetQ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypgPvOWjHJUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the placeholder for our action values\n",
        "\n",
        "X_action = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "Q_action = tf.reduce_sum(input_tensor=targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keepdims=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDMmdLx1HJUz",
        "colab_type": "text"
      },
      "source": [
        "Copy the primary Q network parameters to the target  Q network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdr6lsjmHJU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "copy_op = [tf.compat.v1.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\n",
        "copy_target_to_main = tf.group(*copy_op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ghb6bUyHJU5",
        "colab_type": "text"
      },
      "source": [
        "Compute and optimize loss using gradient descent optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBHokSlGHJU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a placeholder for our output i.e action\n",
        "y = tf.compat.v1.placeholder(tf.float32, shape=(None,1))\n",
        "\n",
        "# now we calculate the loss which is the difference between actual value and predicted value\n",
        "loss = tf.reduce_mean(input_tensor=tf.square(y - Q_action))\n",
        "\n",
        "# we use adam optimizer for minimizing the loss\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "loss_summary = tf.compat.v1.summary.scalar('LOSS', loss)\n",
        "merge_summary = tf.compat.v1.summary.merge_all()\n",
        "file_writer = tf.compat.v1.summary.FileWriter(logdir, tf.compat.v1.get_default_graph())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HOaL7rfHJU_",
        "colab_type": "text"
      },
      "source": [
        " Now we start the tensorflow session and run the model,\n",
        "\n",
        "1. First, we preprocess and feed the game screen (state s) to our DQN, which will\n",
        "return the Q values of all possible actions in the state.\n",
        "2. Now we select an action using the epsilon-greedy policy: with the probability\n",
        "epsilon, we select a random action a and with probability 1-epsilon, we select an\n",
        "action that has a maximum Q value, such as .\n",
        "3. After selecting the action a, we perform this action in a state s and move to a new\n",
        "state s' and receive a reward. The next state, s', is the preprocessed image of the\n",
        "next game screen.\n",
        "4. We store this transition in our replay buffer as <s,a,r,s'>.\n",
        "5. Next, we sample some random batches of transitions from the replay buffer and\n",
        "calculate the loss.\n",
        "\n",
        "6. We know that the loss is defined as the squared\n",
        "difference between target Q and predicted Q.\n",
        "\n",
        "7. We perform gradient descent with respect to our actual network parameters in\n",
        "order to minimize this loss.\n",
        "\n",
        "8. Copy weights of training network to actual network\n",
        "9. Repeat for M steps\n",
        "\n",
        "Epochs here exist because we are suplying data in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "VWMSuB2cHJVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "    init.run()\n",
        "    \n",
        "    # for each episode\n",
        "    history = []\n",
        "    for i in range(num_episodes):\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        epoch = 0\n",
        "        episodic_reward = 0\n",
        "        actions_counter = Counter() \n",
        "        episodic_loss = []\n",
        "\n",
        "        # while the state is not the terminal state\n",
        "        while not done:\n",
        "\n",
        "           #env.render()\n",
        "        \n",
        "            # get the preprocessed game screen\n",
        "            obs = preprocess_observation(obs)\n",
        "\n",
        "            # feed the game screen and get the Q values for each action,  FEED THE NETWORK BY CALLING THE OUTPUT LAYER\n",
        "            \n",
        "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
        "\n",
        "            # get the action\n",
        "            action = np.argmax(actions, axis=-1)\n",
        "            actions_counter[str(action)] += 1 \n",
        "\n",
        "            # select the action using epsilon greedy policy\n",
        "            action = epsilon_greedy(action, global_step)\n",
        "            \n",
        "            # now perform the action and move to the next state, next_obs, receive reward\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store this transistion as an experience in the replay buffer! Quite important\n",
        "            exp_buffer.append([obs, action, preprocess_observation(next_obs), reward, done])\n",
        "            \n",
        "            # After certain steps, we train our Q network with samples from the experience replay buffer\n",
        "            if global_step % steps_train == 0 and global_step > start_steps:\n",
        "                \n",
        "                # sample experience, mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n",
        "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
        "\n",
        "                # states\n",
        "                o_obs = [x for x in o_obs]\n",
        "\n",
        "                # next states\n",
        "                o_next_obs = [x for x in o_next_obs]\n",
        "\n",
        "                # next actions\n",
        "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
        "\n",
        "\n",
        "                # discounted reward: these are our Y-values\n",
        "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done) \n",
        "\n",
        "                # merge all summaries and write to the file\n",
        "                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
        "                file_writer.add_summary(mrg_summary, global_step)\n",
        "\n",
        "                # To calculate the loss, we run the previously defined functions mentioned while feeding inputs\n",
        "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n",
        "                episodic_loss.append(train_loss)\n",
        "            \n",
        "            # after some interval we copy our main Q network weights to target Q network\n",
        "            if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
        "                copy_target_to_main.run()\n",
        "                \n",
        "            obs = next_obs\n",
        "            epoch += 1\n",
        "            global_step += 1\n",
        "            episodic_reward += reward\n",
        "        \n",
        "        history.append(episodic_reward)\n",
        "        print('Epochs per episode:', epoch, 'Episode Reward:', episodic_reward,\"Episode number:\", len(history))\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlDxJthlN7Y0",
        "colab_type": "text"
      },
      "source": [
        "Let's plot our reward distribution across the incremental episodes. Remember to interrupt the training process first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAt912GCIJaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA5JMnuSRSb7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwGRru2gOSGd",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Finally, let's visualize our agent's performance, and play a game within the gym environment itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDxqX-RdK3Nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualization cobe for running within Colab\n",
        "\n",
        "# Install dependencies first for graphics visualization within Colaboratory\n",
        "\n",
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!apt-get update\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "\n",
        "!apt-get install cmake\n",
        "!pip install --upgrade setuptools\n",
        "!pip install ez_setup\n",
        "!apt-get install x11-utils\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA93cHCIIPlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To Evaluate model on OpenAI gym, we will record a video via Ipython display\n",
        "\n",
        "\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcKjTnaqISUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnF5nZirIVQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate model on openAi GYM\n",
        "env = wrap_env(gym.make('MsPacman-v0'))\n",
        "observation = env.reset()\n",
        "new_observation = observation\n",
        "\n",
        "prev_input = None\n",
        "done = False\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    init.run()\n",
        "    while True:\n",
        "      if True: \n",
        "    \n",
        "        #set input to network to be difference image\n",
        "    \n",
        "\n",
        "        obs = preprocess_observation(observation)\n",
        "\n",
        "        # feed the game screen and get the Q values for each action\n",
        "        actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
        "\n",
        "        # get the action\n",
        "        action = np.argmax(actions, axis=-1)\n",
        "        actions_counter[str(action)] += 1 \n",
        "\n",
        "        # select the action using epsilon greedy policy\n",
        "        action = epsilon_greedy(action, global_step)\n",
        "        env.render()\n",
        "        observation = new_observation        \n",
        "        # now perform the action and move to the next state, next_obs, receive reward\n",
        "        new_observation, reward, done, _ = env.step(action)\n",
        "\n",
        "        if done: \n",
        "          #observation = env.reset()\n",
        "          break\n",
        "      \n",
        "    env.close()\n",
        "    show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}